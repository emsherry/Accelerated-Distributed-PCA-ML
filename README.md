# ğŸ“Š Momentum-Accelerated Distributed PCA

This project implements and compares several PCA algorithms for distributed machine learning, including:
- **Centralized Sanger (GHA)**: Standard Generalized Hebbian Algorithm
- **Centralized Momentum-Sanger**: Heavy Ball momentum acceleration
- **Distributed Sanger Algorithm (DSA)**: Consensus-based distributed PCA
- **Momentum-Accelerated DSA (M-DSA)**: Distributed PCA with momentum

> âœ… Designed for AI/ML research on dimensionality reduction, communication-efficient learning, and subspace optimization in distributed environments.

---

## ğŸš€ Key Features

- **Exact Mathematical Implementation**: All algorithms use precise equations from the literature
- **Distributed Architecture**: Supports multi-node consensus-based learning
- **Momentum Acceleration**: Heavy Ball method for faster convergence
- **Real Dataset Support**: MNIST and CIFAR-10 with proper preprocessing
- **Comprehensive Evaluation**: Cosine angle distance metric for subspace quality

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ momentum_experiment.py      # Main experiment script
â”œâ”€â”€ plot_results.py            # Results visualization
â”œâ”€â”€ centralized_pca.py         # Centralized algorithms
â”œâ”€â”€ distributed_pca.py         # Distributed algorithms
â”œâ”€â”€ Algorithms.py              # Legacy algorithm implementations
â”œâ”€â”€ Algorithms_Accelerated.py  # Legacy accelerated implementations
â”œâ”€â”€ Data.py                    # Synthetic data generation
â”œâ”€â”€ read_dataset.py           # Real dataset loading
â”œâ”€â”€ GraphTopology.py          # Network topology generation
â”œâ”€â”€ Datasets/                 # MNIST and CIFAR-10 data
â””â”€â”€ results/                  # Experiment outputs
```

---

## ğŸ“Š Datasets Supported

- ğŸ–¼ï¸ **MNIST**: 784-dimensional vectors (28Ã—28 flattened images)
- ğŸŒˆ **CIFAR-10**: 1024-dimensional vectors (processed RGB images)
- ğŸ§ª **Synthetic**: Configurable dimension, samples, and eigenvalue gap

---

## ğŸ§ª Quick Start

### Basic Experiment
```bash
# Synthetic data experiment
python momentum_experiment.py --dataset synthetic --K 5 --max_iters 1000 --num_nodes 4 --plot

# MNIST experiment
python momentum_experiment.py --dataset mnist --K 5 --max_iters 1000 --num_nodes 4 --limit 2000 --plot

# CIFAR-10 experiment
python momentum_experiment.py --dataset cifar10 --K 10 --max_iters 1000 --num_nodes 6 --limit 3000 --plot
```

### Advanced Configuration
```bash
# Different step sizes for algorithms
python momentum_experiment.py --dataset synthetic --K 5 --max_iters 1000 --num_nodes 4 \
    --alpha_dsa 0.02 --alpha_mdsa 0.01 --beta 0.95 --plot --verbose

# Large-scale experiment
python momentum_experiment.py --dataset synthetic --K 8 --max_iters 2000 --num_nodes 8 \
    --d 100 --N 5000 --eigengap 0.8 --plot
```

### Plot Results
```bash
# Plot saved results
python plot_results.py results/experiment_file.npz --summary
```

---

## ğŸ“ˆ Key Results

**Momentum acceleration consistently improves convergence:**
- **Centralized**: 1-8% improvement with momentum
- **Distributed**: 5-15% improvement with M-DSA vs DSA
- **Real Data**: Best performance on MNIST and CIFAR-10

---

## ğŸ”§ Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or using uv (faster)
uv pip install -r requirements.txt
```

---

## ğŸ§  Research Applications

- **Federated Learning**: Distributed dimensionality reduction
- **Edge Computing**: Efficient PCA on resource-constrained devices
- **IoT Systems**: Scalable subspace learning across sensors
- **Distributed ML**: Communication-efficient optimization

---

## ğŸ“š Algorithm Details

### Centralized Algorithms
- **Sanger (GHA)**: `W_{t+1} = W_t + Î±(CW_t - W_t triu(W_t^T CW_t))`
- **Momentum-Sanger**: `V_{t+1} = Î²V_t + âˆ‡f(W_t)`, `W_{t+1} = W_t + Î±V_{t+1}`

### Distributed Algorithms
- **DSA**: Consensus + Local Sanger update per node
- **M-DSA**: Consensus + Local momentum update per node

### Key Equations
- **Consensus**: `XÌ‚_i = Î£_j W[i,j] X_j`
- **Local Sanger**: `H_i = C_i XÌ‚_i - XÌ‚_i triu(XÌ‚_i^T C_i XÌ‚_i)`
- **Position Update**: `X_i^{t+1} = XÌ‚_i + Î± H_i`
- **Momentum Update**: `V_i^{t+1} = Î² V_i + H_i`, `X_i^{t+1} = XÌ‚_i + Î± V_i^{t+1}`

---

## ğŸ¯ Performance Highlights

- **M-DSA shows 14.92% improvement** over standard DSA
- **Faster convergence** to lower error levels
- **Maintains orthonormality** throughout distributed optimization
- **Proper consensus dynamics** with doubly stochastic mixing matrices

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.